{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline NLP - Analisi e Modellazione Testo\n",
    "\n",
    "Questo notebook contiene tutte le procedure standard per un progetto di Natural Language Processing:\n",
    "- Caricamento e esplorazione dati\n",
    "- Pulizia e preprocessing del testo\n",
    "- Analisi esplorativa\n",
    "- Feature extraction\n",
    "- Modellazione\n",
    "- Valutazione"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Librerie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from pathlib import Path\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Librerie di base caricate con successo!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "print(\"NLTK configurato!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "print(\"Scikit-learn pronto!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configurazione Percorsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = Path('..')\n",
    "DATA_DIR = BASE_DIR / 'data'\n",
    "RAW_DATA_DIR = DATA_DIR / 'raw'\n",
    "PROCESSED_DATA_DIR = DATA_DIR / 'processed'\n",
    "CLEANED_DATA_DIR = DATA_DIR / 'cleaned'\n",
    "MODELS_DIR = BASE_DIR / 'models'\n",
    "\n",
    "for directory in [RAW_DATA_DIR, PROCESSED_DATA_DIR, CLEANED_DATA_DIR, MODELS_DIR]:\n",
    "    directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Cartelle configurate:\")\n",
    "print(f\"  - Dati raw: {RAW_DATA_DIR}\")\n",
    "print(f\"  - Dati processati: {PROCESSED_DATA_DIR}\")\n",
    "print(f\"  - Dati puliti: {CLEANED_DATA_DIR}\")\n",
    "print(f\"  - Modelli: {MODELS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Caricamento Dati\n",
    "\n",
    "Modifica questa cella in base al formato dei tuoi dati (CSV, JSON, Excel, TXT, ecc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(RAW_DATA_DIR / 'nome_file.csv')\n",
    "# df = pd.read_excel(RAW_DATA_DIR / 'nome_file.xlsx')\n",
    "# df = pd.read_json(RAW_DATA_DIR / 'nome_file.json')\n",
    "\n",
    "# Esempio con dati di test\n",
    "df = pd.DataFrame({\n",
    "    'text': [\n",
    "        'Questo è un esempio di testo da analizzare.',\n",
    "        'Il Natural Language Processing è molto utile!',\n",
    "        'Python è ottimo per il machine learning.',\n",
    "    ],\n",
    "    'label': [0, 1, 1]\n",
    "})\n",
    "\n",
    "print(f\"Dataset caricato: {len(df)} righe\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Esplorazione Dati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== INFO DATASET ===\")\n",
    "print(f\"Numero di righe: {len(df)}\")\n",
    "print(f\"Numero di colonne: {len(df.columns)}\")\n",
    "print(f\"\\nColonne: {list(df.columns)}\")\n",
    "print(f\"\\nTipi di dati:\")\n",
    "print(df.dtypes)\n",
    "print(f\"\\nValori mancanti:\")\n",
    "print(df.isnull().sum())\n",
    "print(f\"\\nStatistiche:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modifica 'text' con il nome della colonna che contiene il testo\n",
    "text_column = 'text'\n",
    "\n",
    "if text_column in df.columns:\n",
    "    df['text_length'] = df[text_column].astype(str).str.len()\n",
    "    df['word_count'] = df[text_column].astype(str).str.split().str.len()\n",
    "    \n",
    "    print(f\"\\n=== STATISTICHE TESTO ===\")\n",
    "    print(f\"Lunghezza media: {df['text_length'].mean():.2f} caratteri\")\n",
    "    print(f\"Lunghezza minima: {df['text_length'].min()} caratteri\")\n",
    "    print(f\"Lunghezza massima: {df['text_length'].max()} caratteri\")\n",
    "    print(f\"\\nNumero medio di parole: {df['word_count'].mean():.2f}\")\n",
    "    print(f\"Numero minimo di parole: {df['word_count'].min()}\")\n",
    "    print(f\"Numero massimo di parole: {df['word_count'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "axes[0].hist(df['text_length'], bins=50, edgecolor='black')\n",
    "axes[0].set_title('Distribuzione Lunghezza Testi')\n",
    "axes[0].set_xlabel('Numero di caratteri')\n",
    "axes[0].set_ylabel('Frequenza')\n",
    "\n",
    "axes[1].hist(df['word_count'], bins=50, edgecolor='black', color='orange')\n",
    "axes[1].set_title('Distribuzione Numero Parole')\n",
    "axes[1].set_xlabel('Numero di parole')\n",
    "axes[1].set_ylabel('Frequenza')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se hai una colonna con le etichette/classi\n",
    "label_column = 'label'  # Modifica con il nome della tua colonna\n",
    "\n",
    "if label_column in df.columns:\n",
    "    print(f\"\\n=== DISTRIBUZIONE CLASSI ===\")\n",
    "    print(df[label_column].value_counts())\n",
    "    print(f\"\\nProporzioni:\")\n",
    "    print(df[label_column].value_counts(normalize=True))\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    df[label_column].value_counts().plot(kind='bar')\n",
    "    plt.title('Distribuzione delle Classi')\n",
    "    plt.xlabel('Classe')\n",
    "    plt.ylabel('Frequenza')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Pulizia del Testo\n",
    "\n",
    "Funzioni per pulire e normalizzare il testo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Pulisce il testo:\n",
    "    - Rimuove URL\n",
    "    - Rimuove email\n",
    "    - Rimuove menzioni (@username)\n",
    "    - Rimuove hashtag (#tag)\n",
    "    - Rimuove caratteri speciali e numeri\n",
    "    - Converte in minuscolo\n",
    "    - Rimuove spazi multipli\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return ''\n",
    "    \n",
    "    text = str(text)\n",
    "    \n",
    "    # Rimuovi URL\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Rimuovi email\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    \n",
    "    # Rimuovi menzioni e hashtag\n",
    "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "    \n",
    "    # Rimuovi numeri\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Rimuovi punteggiatura e caratteri speciali\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Converti in minuscolo\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Rimuovi spazi multipli\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "print(\"Funzione clean_text() definita\")\n",
    "\n",
    "# Test\n",
    "sample_text = \"Ciao! Visita https://example.com e contattami a test@email.com #NLP @user123\"\n",
    "print(f\"\\nTesto originale: {sample_text}\")\n",
    "print(f\"Testo pulito: {clean_text(sample_text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applica la pulizia al dataset\n",
    "print(\"Pulizia del testo in corso...\")\n",
    "df['cleaned_text'] = df[text_column].progress_apply(clean_text)\n",
    "\n",
    "# Rimuovi righe con testo vuoto dopo la pulizia\n",
    "df_cleaned = df[df['cleaned_text'].str.len() > 0].copy()\n",
    "\n",
    "print(f\"\\nRighe originali: {len(df)}\")\n",
    "print(f\"Righe dopo pulizia: {len(df_cleaned)}\")\n",
    "print(f\"Righe rimosse: {len(df) - len(df_cleaned)}\")\n",
    "\n",
    "df_cleaned[['text', 'cleaned_text']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Preprocessing Avanzato\n",
    "\n",
    "Tokenizzazione, rimozione stopwords, stemming/lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configura le stopwords (puoi scegliere la lingua)\n",
    "# Lingue disponibili: 'italian', 'english', 'spanish', 'french', 'german', ecc.\n",
    "language = 'italian'  # Modifica in base alla tua lingua\n",
    "\n",
    "try:\n",
    "    stop_words = set(stopwords.words(language))\n",
    "    print(f\"Stopwords caricate per la lingua: {language}\")\n",
    "    print(f\"Numero di stopwords: {len(stop_words)}\")\n",
    "    print(f\"\\nEsempi di stopwords: {list(stop_words)[:10]}\")\n",
    "except:\n",
    "    print(f\"Lingua '{language}' non disponibile, uso l'inglese\")\n",
    "    stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, remove_stopwords=True, use_lemmatization=True):\n",
    "    \"\"\"\n",
    "    Preprocessing del testo:\n",
    "    - Tokenizzazione\n",
    "    - Rimozione stopwords (opzionale)\n",
    "    - Lemmatization o Stemming (opzionale)\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or text == '':\n",
    "        return []\n",
    "    \n",
    "    # Tokenizzazione\n",
    "    tokens = word_tokenize(str(text))\n",
    "    \n",
    "    # Rimuovi stopwords\n",
    "    if remove_stopwords:\n",
    "        tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    \n",
    "    # Lemmatization o Stemming\n",
    "    if use_lemmatization:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    else:\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "print(\"Funzione preprocess_text() definita\")\n",
    "\n",
    "# Test\n",
    "sample = \"Gli studenti stanno studiando per gli esami\"\n",
    "print(f\"\\nTesto: {sample}\")\n",
    "print(f\"Tokens: {preprocess_text(sample)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applica il preprocessing\n",
    "print(\"Preprocessing in corso...\")\n",
    "df_cleaned['tokens'] = df_cleaned['cleaned_text'].progress_apply(\n",
    "    lambda x: preprocess_text(x, remove_stopwords=True, use_lemmatization=True)\n",
    ")\n",
    "\n",
    "# Crea una versione del testo con i token uniti\n",
    "df_cleaned['processed_text'] = df_cleaned['tokens'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Rimuovi righe senza token\n",
    "df_cleaned = df_cleaned[df_cleaned['processed_text'].str.len() > 0].copy()\n",
    "\n",
    "print(f\"\\nRighe dopo preprocessing: {len(df_cleaned)}\")\n",
    "df_cleaned[['cleaned_text', 'processed_text']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analisi Esplorativa Post-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Conta le parole più frequenti\n",
    "all_words = [word for tokens in df_cleaned['tokens'] for word in tokens]\n",
    "word_freq = Counter(all_words)\n",
    "\n",
    "print(f\"Numero totale di parole uniche: {len(word_freq)}\")\n",
    "print(f\"\\nTop 20 parole più frequenti:\")\n",
    "for word, count in word_freq.most_common(20):\n",
    "    print(f\"  {word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizza le parole più frequenti\n",
    "top_words = dict(word_freq.most_common(20))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(top_words.keys(), top_words.values())\n",
    "plt.title('Top 20 Parole Più Frequenti')\n",
    "plt.xlabel('Parola')\n",
    "plt.ylabel('Frequenza')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WordCloud (nuvola di parole)\n",
    "try:\n",
    "    from wordcloud import WordCloud\n",
    "    \n",
    "    all_text = ' '.join(df_cleaned['processed_text'])\n",
    "    \n",
    "    wordcloud = WordCloud(width=800, height=400, \n",
    "                         background_color='white',\n",
    "                         max_words=100).generate(all_text)\n",
    "    \n",
    "    plt.figure(figsize=(15, 8))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title('WordCloud del Corpus', fontsize=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\nexcept ImportError:\n",
    "    print(\"WordCloud non disponibile. Installa con: pip install wordcloud\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Salvataggio Dati Processati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salva il dataset pulito\n",
    "output_file = CLEANED_DATA_DIR / 'data_cleaned.csv'\n",
    "df_cleaned.to_csv(output_file, index=False)\n",
    "print(f\"Dataset pulito salvato in: {output_file}\")\n",
    "\n",
    "# Salva anche in formato pickle per preservare i tipi di dato\n",
    "output_pickle = CLEANED_DATA_DIR / 'data_cleaned.pkl'\n",
    "df_cleaned.to_pickle(output_pickle)\n",
    "print(f\"Dataset salvato anche in: {output_pickle}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Feature Extraction\n",
    "\n",
    "Trasforma il testo in features numeriche per il machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 Bag of Words (BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer - Bag of Words\n",
    "count_vectorizer = CountVectorizer(max_features=100, min_df=2)\n",
    "bow_features = count_vectorizer.fit_transform(df_cleaned['processed_text'])\n",
    "\n",
    "print(f\"Shape matrice BoW: {bow_features.shape}\")\n",
    "print(f\"Numero di features: {len(count_vectorizer.get_feature_names_out())}\")\n",
    "print(f\"\\nEsempio features: {list(count_vectorizer.get_feature_names_out()[:20])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=100, min_df=2)\n",
    "tfidf_features = tfidf_vectorizer.fit_transform(df_cleaned['processed_text'])\n",
    "\n",
    "print(f\"Shape matrice TF-IDF: {tfidf_features.shape}\")\n",
    "print(f\"Numero di features: {len(tfidf_vectorizer.get_feature_names_out())}\")\n",
    "print(f\"\\nEsempio features: {list(tfidf_vectorizer.get_feature_names_out()[:20])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Preparazione per Modellazione\n",
    "\n",
    "Split train/test per supervisionati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se hai una colonna target per classificazione/regressione\n",
    "if label_column in df_cleaned.columns:\n",
    "    X = tfidf_features  # o bow_features\n",
    "    y = df_cleaned[label_column]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n=== SPLIT DATASET ===\")\n",
    "    print(f\"Training set: {X_train.shape[0]} campioni\")\n",
    "    print(f\"Test set: {X_test.shape[0]} campioni\")\n",
    "    print(f\"\\nDistribuzione classi nel training set:\")\n",
    "    print(y_train.value_counts())\n",
    "    print(f\"\\nDistribuzione classi nel test set:\")\n",
    "    print(y_test.value_counts())\nelse:\n",
    "    print(\"Nessuna colonna target trovata - preparazione per unsupervised learning\")\n",
    "    X = tfidf_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Modellazione Base\n",
    "\n",
    "Esempi di modelli semplici da provare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esempio: Naive Bayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "if label_column in df_cleaned.columns:\n",
    "    print(\"Training Naive Bayes...\")\n",
    "    nb_model = MultinomialNB()\n",
    "    nb_model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = nb_model.predict(X_test)\n",
    "    \n",
    "    print(f\"\\nAccuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"\\nClassification Report:\\n{classification_report(y_test, y_pred)}\")\nelse:\n",
    "    print(\"Modello supervisionato non applicabile - nessuna colonna target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esempio: Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "if label_column in df_cleaned.columns:\n",
    "    print(\"Training Logistic Regression...\")\n",
    "    lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "    lr_model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = lr_model.predict(X_test)\n",
    "    \n",
    "    print(f\"\\nAccuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"\\nClassification Report:\\n{classification_report(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "if label_column in df_cleaned.columns:\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Salvataggio Modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Salva il modello e il vectorizer\n",
    "if label_column in df_cleaned.columns:\n",
    "    with open(MODELS_DIR / 'model.pkl', 'wb') as f:\n",
    "        pickle.dump(lr_model, f)\n",
    "    \n",
    "    with open(MODELS_DIR / 'vectorizer.pkl', 'wb') as f:\n",
    "        pickle.dump(tfidf_vectorizer, f)\n",
    "    \n",
    "    print(f\"Modello salvato in: {MODELS_DIR / 'model.pkl'}\")\n",
    "    print(f\"Vectorizer salvato in: {MODELS_DIR / 'vectorizer.pkl'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Note e Prossimi Passi\n",
    "\n",
    "### Cosa puoi fare ora:\n",
    "\n",
    "1. **Carica i tuoi dati** nella sezione 3\n",
    "2. **Adatta le funzioni di pulizia** alle tue esigenze (sezione 5)\n",
    "3. **Scegli la lingua** per le stopwords (sezione 6)\n",
    "4. **Prova diversi modelli**:\n",
    "   - Naive Bayes\n",
    "   - Logistic Regression\n",
    "   - SVM\n",
    "   - Random Forest\n",
    "   - XGBoost\n",
    "   - Deep Learning (LSTM, Transformers)\n",
    "\n",
    "5. **Tecniche avanzate da esplorare**:\n",
    "   - Word Embeddings (Word2Vec, GloVe, FastText)\n",
    "   - Transfer Learning (BERT, GPT)\n",
    "   - Topic Modeling (LDA)\n",
    "   - Named Entity Recognition (NER)\n",
    "   - Sentiment Analysis\n",
    "   - Text Generation\n",
    "\n",
    "6. **Ottimizzazione**:\n",
    "   - Hyperparameter tuning (GridSearch, RandomSearch)\n",
    "   - Cross-validation\n",
    "   - Feature engineering avanzato\n",
    "   - Ensemble methods"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
